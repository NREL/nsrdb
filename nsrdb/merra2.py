# -*- coding: utf-8 -*-
"""Interpolates ancillary data from various sources into 4x4km daily and 1/2
hourly data

The following variables can be processed using this module:
    - PS (surface_pressure, Pa)
    - T2M (air_temperature, C)
    - TO3 (ozone, atm-cm)
    - TQV (total_precipitable_water, cm)
    - wind_speed (m/s)
    - wind_direction (degrees)
    - QV2M (specific_humidity, kg_water/kg_air)
    - relative_humidity (%)
    - dew_point (C)
    - TOTANGSTR (alpha, angstrom wavelength exponent, unitless)
    - TOTEXTTAU (aod, aerosol optical depth, unitless)
    - TOTSCATAU (ssa, aerosol single scatter albedo, unitless)
    - asymmetry (Aerosol asymmetry parameter, unitless)
    - surface_albedo (unitless)
"""

import numpy as np
import pandas as pd
import pyproj
import h5py
import time
import sys
import os
from scipy.spatial import cKDTree
from configobj import ConfigObj
from netCDF4 import Dataset
import logging
import re
import pprint
from datetime import date
from dask.distributed import LocalCluster, Client

from nsrdb.utilities.loggers import NSRDB_LOGGERS
from nsrdb.utilities.execution import PBS

logger = logging.getLogger(__name__)


def idw(data, distance, indices, p):
    """Inverse Distance Weighted Interpolation.

    Parameters
    ----------
    data : np.ndarray
    distance : np.ndarray
        Distance array generated by a cKDTree query.
    indices : np.ndarray
        Index array generated by a cKDTree query.
    p : int
        Power of the distance. Default = 2.

    Returns
    -------
    out : np.ndarray
    """

    weight = 1.0 / np.power(distance, p)
    # Need to account for divide by zero which is result of perfect overlap.
    # Note numpy doesn't doesn't bomb on divide by zero, it instead returns NaN
    rows = np.where(distance == 0)[0]
    weight[rows, 1:] = 0
    weight[rows, 0] = 1
    data_idw = (np.sum(weight * data[indices], axis=1) /
                np.sum(weight, axis=1))
    return data_idw


def nn(data, indices):
    """Nearest Neighbor Interpolation.

    Parameters
    -------------
    data : np.ndarray
    indices : np.ndarray
        Index array generated by a cKDTree query.

    Returns
    -------
    out : np.ndarray
    """
    data_nn = data[indices]
    return data_nn


def projectLL(ref_grid, inverse=False):
    """
    Convert Lat Lng to North America Equidistant Conic or the Reverse.
    Defualt is to project
    Source: http://spatialreference.org/ref/esri/102010/
    """
    lng, lat = ref_grid['longitude'].values, ref_grid['latitude'].values
    p = pyproj.Proj('+proj=eqdc +lat_0=0 +lon_0=0 +lat_1=20 +lat_2=60 +x_0=0 '
                    '+y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs')
    x, y = p(lng, lat, inverse=inverse)
    return np.dstack((x, y))[0]


def createOutFile(config, dset, var, region, year, ref_grid,
                  chunks=(48, 1000)):
    """Create a new .h5 file for ancillary variable output.

    Parameters
    ----------
    config : ConfigObj
        Config .ini containing required Ancillary variable processing
        inputs.
    dset : str
        Source dataset. This dset should be the name of a folder in
        the directory: config['ancillary']['ancil_dir']. Examples:
            - tavg1_2d_aer_Nx
            - tavg1_2d_slv_Nx
    var : str
        Ancillary variable name to process.
        Examples for each dset with final units:
            - tavg1_2d_slv_Nx
                - PS (surface_pressure, Pa)
                - T2M (air_temperature, C)
                - TO3 (ozone, atm-cm)
                - TQV (total_precipitable_water, cm)
                - wind_speed (m/s)
                - wind_direction (degrees)
                - QV2M (specific_humidity, kg_water/kg_air)
                - relative_humidity (%)
                - dew_point (C)
            - tavg1_2d_aer_Nx
                - TOTANGSTR (alpha, angstrom wavelength exponent, unitless)
                - TOTEXTTAU (aod, aerosol optical depth, unitless)
                - TOTSCATAU (ssa, aerosol single scatter albedo, unitless)
    region : str
        East or west.
    year : int | str
        Analysis year.
    ref_grid : pd.DataFrame
        Analysis reference grid. This is taken from the file:
        '{region}_psm_extent.csv' located in:
        config['ancillary']['reference_dir']
    chunks : list | tuple
        Chunk size in the output hdf.

    Returns
    -------
    fname_path : str
        Name and path to the created h5 file.
    """
    if var in ['asymmetry', 'surface_albedo']:
        var_config = config['ancillary'][var]
    else:
        var_config = config['ancillary'][dset][var]

    # name of the output file
    fname = ('{var}_{region}_{year}.h5'
             .format(var=var_config['psm_name'],
                     region=region, year=year))
    fname_path = os.path.join(config['ancillary']['out_dir'], fname)
    # name of the output dataset for PSM
    psm_name = var_config['psm_name']
    if not os.path.exists(fname_path):
        # create the HDF
        with h5py.File(fname_path, 'w') as out:
            # create the time index
            ti = pd.date_range('1-1-{year}'.format(year=year),
                               '1-1-{year}'.format(year=year + 1),
                               freq='30min')[:-1].values.astype('S30')
            out.create_dataset('time_index', data=ti)
            out['time_index'].attrs['timezone'] = 'UTC'

            # create the empty dataset with PSM name and units
            out.create_dataset(psm_name,
                               shape=(out['time_index'].shape[0],
                                      len(ref_grid)),
                               chunks=chunks,
                               dtype=eval(var_config['psm_dtype']))
            # add PSM units
            out[psm_name].attrs['units'] = var_config['psm_units']
            # add PSM scale factor
            out[psm_name].attrs['psm_scale_factor'] = \
                var_config['psm_scale_factor']

            # meta dataset
            meta = np.dtype([('latitude', np.float),
                             ('longitude', np.float),
                             ('mean_var', np.float)])
            hset = out.create_dataset('meta', shape=(len(ref_grid), ),
                                      dtype=meta)
            hset['longitude'] = ref_grid['longitude']
            hset['latitude'] = ref_grid['latitude']

    return fname_path


class Ancillary(object):
    """Class to handle processing of ancillary variables from source to hdf5.
    """

    # Variable to dataset mapping
    DSETS = {'PS': 'tavg1_2d_slv_Nx',
             'T2M': 'tavg1_2d_slv_Nx',
             'TO3': 'tavg1_2d_slv_Nx',
             'TQV': 'tavg1_2d_slv_Nx',
             'wind_speed': 'tavg1_2d_slv_Nx',
             'wind_direction': 'tavg1_2d_slv_Nx',
             'QV2M': 'tavg1_2d_slv_Nx',
             'relative_humidity': 'tavg1_2d_slv_Nx',
             'dew_point': 'tavg1_2d_slv_Nx',
             'TOTANGSTR': 'tavg1_2d_aer_Nx',
             'TOTEXTTAU': 'tavg1_2d_aer_Nx',
             'TOTSCATAU': 'tavg1_2d_aer_Nx',
             'asymmetry': 'tavg1_2d_aer_Nx',
             'surface_albedo': 'tavg1_2d_aer_Nx',
             }

    def __init__(self, config, var, year, region, cores, date_range):
        """Initialize an ancillary variable processing instance.

        Parameters
        ----------
        config : ConfigObj
            Config .ini containing required Ancillary variable processing
            inputs.
        var : str
            Ancillary variable name to process.
            Examples for each dset with final units:
                - tavg1_2d_slv_Nx
                    - PS (surface_pressure, Pa)
                    - T2M (air_temperature, C)
                    - TO3 (ozone, atm-cm)
                    - TQV (total_precipitable_water, cm)
                    - wind_speed (m/s)
                    - wind_direction (degrees)
                    - QV2M (specific_humidity, kg_water/kg_air)
                    - relative_humidity (%)
                    - dew_point (C)
                - tavg1_2d_aer_Nx
                    - TOTANGSTR (alpha, angstrom wavelength exponent, unitless)
                    - TOTEXTTAU (aod, aerosol optical depth, unitless)
                    - TOTSCATAU (ssa, aerosol single scatter albedo, unitless)
                    - asymmetry (Aerosol asymmetry parameter, unitless)
                    - surface_albedo (unitless)
        year : int
            Analysis year.
        region : str
            East or west.
        cores : int
            Number of cores to process data on.
        date_range : tuple | list
            Two-entry tuple/list with each entry being a datetime.date object.
            Entries are INCLUSIVE, EXCLUSIVE;
            (date(2019, 1, 1), date(2020, 1, 1)) is the entire year range.
        """
        self.config = config
        self.var = var
        self.dset = self.DSETS[self.var]
        self.year = year
        self.region = region
        self.cores = cores
        self.date_range = date_range
        self.psm_name = self.var_config['psm_name']
        self.psm_scale_factor = self.var_config['psm_scale_factor']
        self.psm_dtype = eval(self.var_config['psm_dtype'])
        self.elev_lower = self.var_config['elev_lower']
        self.spatial_interp_method = self.var_config['spatial_interp_method']
        self.temporal_interp_method = self.var_config['temporal_interp_method']
        self.source_index = None
        self.target_index = None
        self.files = None
        self.ref_grid = None
        self.ref_prj = None
        self.ref_elev = None
        self.var_prj = None
        self.var_elev = None
        self.d_idw = None
        self.i_idw = None
        self.d_nn = None
        self.i_nn = None
        self.data = None
        self._get_files()
        self._define_grids()
        self._spatial_indices()
        self._define_time_index()
        self._get_weights()

    @property
    def var_config(self):
        """Get the variable-specific config namespace:
        config['ancillary'][self.dset][self.var]
        """
        if self.var in ['asymmetry', 'surface_albedo']:
            return self.config['ancillary'][self.var]
        else:
            return self.config['ancillary'][self.dset][self.var]

    def _get_weights(self):
        """Get the irradiance model weights for AOD/Alhpa."""
        if self.var == 'TOTANGSTR':
            # Alpha
            weight_var = 'alpha_weights'
        else:
            # AOD
            weight_var = 'aod_weights'

        self.weights = pd.read_csv(
            self.config['irradiance_model'][weight_var],
            sep=' ', skiprows=4, skipinitialspace=1)

        w_lat = self.weights['Lat.'].values
        w_lon = self.weights['Long.'].values
        w_prj = projectLL(pd.DataFrame({'longitude': w_lon,
                                        'latitude': w_lat}))

        tree = cKDTree(w_prj)
        i_nn, _ = tree.query(self.var_prj, k=1)

        df_w = self.weights.iloc[i_nn]
        self.df_w = df_w[df_w.columns[2:-1]].T.set_index(
            pd.date_range(str(self.year), freq='M', periods=12))
        self.df_w[self.df_w < 0] = 1

    def _lowerData(self, data):
        """
        Lower Data Values using Elevation and Scale Height
        scale height 2500 meters (per Chris email dated 12/29/2013 11:12pm)
        """
        if self.psm_name == 'air_temperature':
            const = 6.5 / 1000.
            data = data + self.var_elev * const
        elif self.psm_name == 'surface_pressure':
            const = 101325 * (1 - (np.power((1 - self.var_elev / 44307.69231),
                                            5.25328)))
            data = data + const
            if np.min(data) < 0.0:
                raise ValueError('Spatial interpolation of surface pressure '
                                 'resulted in negative values. Incorrectly '
                                 'scaled/unscaled values or incorrect units '
                                 'are the most likely causes.')
        else:
            scale_height = 2500
            elev_scale = np.exp(self.var_elev / scale_height)
            data = data * elev_scale
        return data

    def _raiseData(self, data):
        """
        Raise Data Values using Elevation and Scale Height
        scale height 2500 meters (per Chris email dated 12/29/2013 11:12pm)
        """
        if self.psm_name == 'air_temperature':
            const = 6.5 / 1000.
            data = data - self.ref_elev * const
        elif self.psm_name == 'surface_pressure':
            const = 101325 * (1 - (np.power((1 - self.ref_elev / 44307.69231),
                                            5.25328)))
            data = data - const
            if np.min(data) < 0.0:
                raise ValueError('Spatial interpolation of surface pressure '
                                 'resulted in negative values. Incorrectly '
                                 'scaled/unscaled values or incorrect units '
                                 'are the most likely causes.')
        else:
            scale_height = 2500
            elev_scale = np.exp(-self.ref_elev / scale_height)
            data = data * elev_scale
        return data

    @property
    def output_range(self):
        """Define the output time axis (y) index range based on full year and
        input subset date_range"""
        if not hasattr(self, '_output_range'):
            out_freq = self.var_config['out_freq']
            ti_full = pd.date_range('1-1-{y}'.format(y=self.year),
                                    '1-1-{y}'.format(y=self.year + 1),
                                    freq=out_freq)
            loc0 = int(np.where(
                ti_full == pd.Timestamp(self.date_range[0]))[0])
            loc1 = int(np.where(
                ti_full == pd.Timestamp(self.date_range[1]))[0])
            self._output_range = range(loc0, loc1)
        return self._output_range

    def _define_time_index(self):
        """Define the time index based on the input arg date_range and
        input/output temporal frequency. """
        inp_freq = self.var_config['in_freq']
        out_freq = self.var_config['out_freq']
        self.source_index = pd.date_range(self.date_range[0],
                                          self.date_range[1],
                                          freq=inp_freq)[:-1]
        self.target_index = pd.date_range(self.date_range[0],
                                          self.date_range[1],
                                          freq=out_freq)[:-1]

    @staticmethod
    def get_timestamp(s):
        """Search the filename for a YYYYMMDD string and return as a dict.

        Parameters
        ----------
        s : str | int
            Filename possibly containing YYYYMMDD string.

        Returns
        -------
        out : dict
            Dictionary with year, month, day keys and integer values. Values
            are None if string is not matched.
        """
        match = re.match(r'.*([1-2][0-9]{3}[0-1][0-9][0-3][0-9])', str(s))
        out = {'year': None, 'month': None, 'day': None, 'date': None}
        if match:
            timestamp = str(int(match.group(1)))
            out['year'] = int(timestamp[0:4])
            out['month'] = int(timestamp[4:6])
            out['day'] = int(timestamp[6:])
            out['date'] = date(out['year'], out['month'], out['day'])
        return out

    def _get_files(self):
        """create list of files to work with.

        Files in config['ancillary']['ancil_dir'] will be included if the
        applicable year is in the filename and the date in the filename
        is in the date_range attribute.
        """
        files = os.listdir(os.path.join(self.config['ancillary']['ancil_dir'],
                                        self.dset))
        self.files = []
        for x in files:
            timestamp = self.get_timestamp(x)
            if str(self.year) in str(timestamp['year']):
                # ensure that the file is in the applicable year
                if (timestamp['date'] >= self.date_range[0] and
                        timestamp['date'] < self.date_range[1]):
                    # check to see if file is in desired date_range
                    self.files.append(x)

    def _define_grids(self):
        # # Get LL of Target Coordinates
        target_file = (self.config['ancillary']['reference_dir'] +
                       '{region}_psm_extent.csv'.format(region=self.region))
        self.ref_grid = pd.read_csv(target_file)
        # # Project to Equadistant
        self.ref_prj = projectLL(self.ref_grid)
        self.ref_elev = self.ref_grid['elevation'].values

        # grab initial dataset to get coords from
        target_file = os.path.join(self.config['ancillary']['ancil_dir'],
                                   self.dset, self.files[0])
        with Dataset(target_file, 'r') as nc:
            lon2d, lat2d = np.meshgrid(nc['lon'][:], nc['lat'][:])
            self.var_grid = pd.DataFrame({'longitude': lon2d.ravel(),
                                          'latitude': lat2d.ravel()})
            # merra grid has some wonky values around 0 lat/lon... quick fix...
            self.var_grid.loc[(self.var_grid['latitude'] > -0.1) &
                              (self.var_grid['latitude'] < 0.1),
                              'latitude'] = 0
            self.var_grid.loc[(self.var_grid['longitude'] > -0.1) &
                              (self.var_grid['longitude'] < 0.1),
                              'longitude'] = 0
            self.var_prj = projectLL(self.var_grid)
            merra_elev = pd.read_pickle(self.config['ancillary']['merra_elev'])
            merged = self.var_grid.merge(merra_elev,
                                         on=['latitude', 'longitude'],
                                         how='left')
            self.var_elev = merged['mean_elevation'].values

    def _spatial_indices(self):
        # get the distance and indices of ref grids to source data
        tree = cKDTree(self.var_prj)
        self.d_idw, self.i_idw = tree.query(self.ref_prj, k=4)
        self.d_nn, self.i_nn = tree.query(self.ref_prj, k=1)

    def _unit_convert(self, data):
        # convert MERRA2 units to PSM units
        if self.var in ['T2M', 'dew_point']:
            # convert Kelvin to Celsius
            data = data - 273.15
        if self.var == 'TO3':
            # convert Dobson to atm-cm
            data = data / 1000.
        if self.var == 'TQV':
            # Convert precip from kg/m2 to cm
            data = data * 0.1
        return data

    def spatial_interpolation(self, core):
        """Execute the spatial interp from MERRA2 source grid to NSRDB grid.

        Parameters
        ----------
        core : int
            Core/worker number being executed. The files to process (each file
            is a single day) are split between the number of cores/workers.
            Each core/worker will process its list of files in series.

        Returns
        -------
        out : pd.DataFrame
            Summary of spatial interpolation outputs. Columns:
                file : int
                    Array of the source MERRA2 file times (YYYYMMDD)
                hfile : str
                    Pointer to h5 file containing the processed MERRA2 data
                    from this core (Each core/worker spits out one hfile
                    containing processed data from all files in the file column
                    of this output df).
        """

        df_times = []
        # get list of files
        process_files = np.array_split(self.files, self.cores)[core]
        # need to store in a temp HDF due to high memory use
        hname = (self.config['ancillary']['temp_dir'] +
                 '{c}_{v}_{r}_{y}_tmp.h5'.format(c=core, v=self.var,
                                                 r=self.region, y=self.year))
        with h5py.File(hname, 'w') as hfile:
            for file in process_files:
                # name
                file_time = file[-16: -8]
                file_month = int(file_time[4:6])
                logger.debug('Spatial interp on core #{} for file {}, '
                             'file_time {}, file_month {}'
                             .format(core, file, file_time, file_month))
                try:
                    mask = (self.df_w.index.month == file_month)
                    weights = self.df_w[mask].values[0]
                    target_file = os.path.join(
                        self.config['ancillary']['ancil_dir'], self.dset, file)
                    with Dataset(target_file, 'r') as nc:
                        # depending on variable, might need extra logic
                        if self.var in ['wind_speed', 'wind_direction']:
                            u_vector = nc['U2M'][:]
                            v_vector = nc['V2M'][:]
                            if self.var == 'wind_speed':
                                data = np.sqrt(u_vector**2 + v_vector**2)
                            else:
                                data = np.degrees(
                                    np.arctan2(u_vector, v_vector)) + 180
                        elif self.var == 'TOTSCATAU':
                            aod = nc['TOTEXTTAU'][:]
                            total_scatter = nc[self.var][:]
                            data = total_scatter / aod
                        else:
                            data = nc[self.var][:]

                        out_array = np.zeros(shape=(data.shape[0],
                                                    len(self.i_nn)))

                        for i in range(data.shape[0]):
                            sub_data = data[i, :, :].ravel()
                            if self.var in ['TOTEXTTAU', 'TOTANGSTR']:
                                sub_data = sub_data * weights
                            if self.elev_lower:
                                sub_data = self._lowerData(sub_data)
                            if self.spatial_interp_method == 'IDW':
                                sub_data = idw(sub_data, self.d_idw,
                                               self.i_idw, p=2)
                            elif self.spatial_interp_method == 'NN':
                                sub_data = nn(sub_data, self.i_nn)
                            if self.elev_lower:
                                sub_data = self._raiseData(sub_data)
                            out_array[i, :] = sub_data
                    # convert units to PSM units
                    out_array = self._unit_convert(out_array)
                    # apply scaling factor and dtype conversion
                    out_array = out_array * self.psm_scale_factor
                    # convert to PSM dtype
                    out_array = out_array.astype(self.psm_dtype)
                    # stuff results HDF file
                    chunks = (np.min((24, out_array.shape[0])),
                              np.min((9997, out_array.shape[1])))
                    hfile.create_dataset(file_time, data=out_array,
                                         chunks=chunks, dtype=self.psm_dtype)
                    # to keep track of processed files
                    df_times.append(int(file_time))
                except Exception as e:
                    _, _, exc_tb = sys.exc_info()
                    logger.exception('Error: {e} At Line: {f}.'
                                     .format(e=e, f=exc_tb.tb_lineno))

        return pd.DataFrame({'file': np.array(df_times), 'hfile': hname})

    @staticmethod
    def temporal_iterpolation(df_results, idx, scale_factor, source_index,
                              target_index, interp_method):
        """Execute the temporal interpolation on a subset of spatial indices.

        This must be a static method to be pickled MUCH faster for
        parallelization.

        Parameters
        ----------
        df_results : pd.DataFrame
            Single df containing concatenated outputs from
            spatial_interpolation method above.
        idx : list
            Spatial indices that this core will process.
        scale_factor : int | float
            Data scale factor. Data is unscaled from the spatial_interp output
            files, processed, re-scaled, then returned.
        source_index : pd.date_range
            MERRA2 source time series time index.
        target_index : pd.date_range
            NSRDB output series time index.
        interp_method : str
            Temporal interpolation method for pandas interpolation
            (linear, nearest).

        Returns
        -------
        data : pd.DataFrame
            Scaled timeseries data with new high res target time index.
            Includes all spatial indices in idx.
        """

        data = []
        # read the spatial interp results
        for _, row in df_results.iterrows():
            with h5py.File(row.hfile, 'r') as hfile:
                data.append(
                    hfile[str(int(row.file))][:, idx.min(): idx.max() + 1])
        # stack the data in matrix
        data = np.vstack(data)
        # apply scale factor
        data = data / scale_factor
        # create time series index
        data = pd.DataFrame(data, index=source_index)
        # perform temperal interp
        data = data.reindex(target_index).interpolate(
            method=interp_method).fillna(method='ffill')
        # recompress with scale factor
        data = data * scale_factor
        return data

    def dewPoint_relHum(self):
        """Calculate the dew point and relative humidity based on the already
        processed air temp, specific humidity, and surface pressure.

        Spatial interpolation is not required for dew point and relative
        humidity because these variables are calcualted from already
        interpolated variables.
        """
        all_data = []
        ids = np.arange(len(self.ref_grid))
        chunks = np.array_split(ids, (len(ids) / np.min((10000, len(ids)))))
        logger.debug(len(chunks))
        for i, chunk in enumerate(chunks):
            logger.debug(i)
            # Temperature: Kelvin
            file_t = (self.config['ancillary']['out_dir'] +
                      'air_temperature_{r}_{y}.h5'
                      .format(r=self.region, y=self.year))
            file_h = (self.config['ancillary']['out_dir'] +
                      'specific_humidity_{r}_{y}.h5'
                      .format(r=self.region, y=self.year))
            file_p = (self.config['ancillary']['out_dir'] +
                      'surface_pressure_{r}_{y}.h5'
                      .format(r=self.region, y=self.year))

            for f in [file_t, file_h, file_p]:
                if not os.path.exists(f):
                    m = ('Air temp, specific humidity, and surface pressure '
                         'must be processed before dew point and relative '
                         'humidity. Could not find the following file: {}'
                         .format(f))
                    raise IOError(m)

            with h5py.File(file_t, 'r') as hfile:
                t = (hfile['air_temperature'][:, chunk] /
                     hfile['air_temperature'].attrs['psm_scale_factor'] +
                     273.15)
            # Specific Humidity: kg kg-1
            with h5py.File(file_h, 'r') as hfile:
                q = (hfile['specific_humidity'][:, chunk] /
                     hfile['specific_humidity'].attrs['psm_scale_factor'])
            # Pressure: Pa
            with h5py.File(file_p, 'r') as hfile:
                psta = (hfile['surface_pressure'][:, chunk] /
                        hfile['surface_pressure'].attrs['psm_scale_factor'])

            # determine ps
            ps = 610.79 * np.exp((t - 273.15) / (t - 273.15 + 238.3) * 17.2694)
            # determine w
            w = q / (1 - q)
            # determine ws
            ws = 621.97 * (ps / 1000.) / (psta - (ps / 1000.))
            # determine RH
            rh = w / ws * 100.
            # check values
            rh[rh > 100] = 100
            rh[rh < 2] = 2

            if self.psm_name == 'relative_humidity':
                data = rh
            else:
                # dew point requires temp to be in Celsius
                t = t - 273.15
                data = (243.04 * (np.log(rh / 100.) +
                                  ((17.625 * t) / (243.04 + t))) /
                        (17.625 - np.log(rh / 100.) -
                         ((17.625 * t) / (243.04 + t))))
            all_data.append(
                (data * self.psm_scale_factor).astype(self.psm_dtype))
        return np.hstack(all_data)


def run_single(config_file, date_range, region, var, cores=1,
               test=False):
    """Run the ancillary variable processing for a single year/var/region.

    Parameters
    ----------
    config_file : str
        Pointer to a config .ini file containing required Ancillary variable
        processing inputs.
    date_range : tuple | list
        Two-entry tuple/list with each entry being a datetime.date object
        or an integer of format YYYYMMDD. Entries are (INCLUSIVE, EXCLUSIVE).
        (date(2019, 1, 1), date(2020, 1, 1)) is the entire year range.
    region : str
        East or west.
    var : str
        Ancillary variable name to process.
        Examples for each dset with final units:
            - tavg1_2d_slv_Nx
                - PS (surface_pressure, Pa)
                - T2M (air_temperature, C)
                - TO3 (ozone, atm-cm)
                - TQV (total_precipitable_water, cm)
                - wind_speed (m/s)
                - wind_direction (degrees)
                - QV2M (specific_humidity, kg_water/kg_air)
                - relative_humidity (%)
                - dew_point (C)
            - tavg1_2d_aer_Nx
                - TOTANGSTR (alpha, angstrom wavelength exponent, unitless)
                - TOTEXTTAU (aod, aerosol optical depth, unitless)
                - TOTSCATAU (ssa, aerosol single scatter albedo, unitless)
                - asymmetry (Aerosol asymmetry parameter, unitless)
                - surface_albedo (unitless)
    cores : int
        Number of cores to process data on.
    test : bool
        Activates a test routine.
    """

    NSRDB_LOGGERS.init_logger(__name__)

    logger.info('MERRA2 processing var "{}" over date range {} in region "{}"'
                .format(var, date_range, region))
    logger.info('The following config file is being used: {}'
                .format(config_file))

    config = ConfigObj(config_file, unrepr=True)

    logger.debug('Running MERRA2 processing with the following config input:\n'
                 '{}'.format(pprint.pformat(config, indent=4)))

    t1 = time.time()
    if len(date_range) != 2:
        raise Exception('Date range must be a 2 entry tuple/list')
    else:
        if isinstance(date_range[0], int) and isinstance(date_range[1], int):
            date_range = (Ancillary.get_timestamp(date_range[0])['date'],
                          Ancillary.get_timestamp(date_range[1])['date'])
        elif (not isinstance(date_range[0], date) or
                not isinstance(date_range[1], date)):
            raise TypeError('date_range input must be a list of integers or '
                            'date objects. Received: {} {}'
                            .format(date_range, type(date_range[0])))
    year = date_range[0].year

    logger.info('Gathering info and setting up spatial interpolation...')
    ancil = Ancillary(config, var, year, region, cores, date_range)

    s1 = time.time()

    logger.info('Performing Spatial Interpolation...')
    if test == 'True':
        results = ancil.spatial_interpolation(0)
    else:
        if ancil.psm_name in ['dew_point', 'relative_humidity']:
            # Spatial interpolation is not required for dew point and relative
            # humidity because these variables are calcualted from already
            # interpolated variables.
            results = ancil.dewPoint_relHum()
        else:
            workers = np.min((len(ancil.files), cores))
            if workers > 1:
                logger.debug('Running spatial interp in parallel.')
                # run in parallel
                cluster = LocalCluster(n_workers=workers)
                futures = []
                with Client(cluster) as client:
                    client.run(NSRDB_LOGGERS.init_logger, __name__)
                    for i in range(workers):
                        futures.append(client.submit(
                            ancil.spatial_interpolation, i))
                    results = client.gather(futures)
            else:
                # run in serial
                logger.debug('Running spatial interp in serial.')
                results = [ancil.spatial_interpolation(0)]

            logger.info('Spatial interp results contain {} futures'
                        .format(len(results)))

    logger.info('Spatial Interpolation: {} min'
                .format((time.time() - s1) / 60.))

    # create the output HDF and return the name and path
    outHDF = createOutFile(config, ancil.dset, var, region, year,
                           ancil.ref_grid)

    if ancil.psm_name in ['dew_point', 'relative_humidity']:
        with h5py.File(outHDF, 'a') as f:
            f[ancil.psm_name][:] = results

            # calculate annual means as summary
            summary = results.mean(axis=0)

            # save the summary data to csv.
            pd.DataFrame({'lat': f['meta']['latitude'][:],
                          'lon': f['meta']['longitude'][:],
                          'data': summary}).to_csv(
                              config['ancillary']['out_dir'] +
                              '{v}_{r}_{y}.csv'.format(v=ancil.psm_name,
                                                       r=region, y=year))
    else:
        # perform temporal interp
        df_results = pd.concat(results)
        # sort results so in correct time-series order
        df_results.sort_values(by='file', inplace=True)
        # for storing summary stats
        summary = []
        # process N number of pixels at once
        chunks = np.array_split(np.arange(len(ancil.ref_grid)),
                                np.round(len(ancil.ref_grid) / 500.0))
        logger.info('Kicking off temporal interp for ref grid with length {} '
                    'in {} chunks'.format(len(ancil.ref_grid), len(chunks)))
        s1 = time.time()

        if cores > 1:
            cluster = LocalCluster(n_workers=cores)

        for n_chunk, idx in enumerate(chunks):
            logger.debug('Kicking off temporal interp for chunk #{} with '
                         'spatial indices {} through {}.'
                         .format(n_chunk, idx[0], idx[-1]))
            e1 = time.time()
            try:
                sub_chunks = np.array_split(idx, cores)

                if cores > 1:
                    futures = []
                    with Client(cluster) as client:
                        client.run(NSRDB_LOGGERS.init_logger, __name__)
                        for i, sub_idx in enumerate(sub_chunks):
                            futures.append(client.submit(
                                ancil.temporal_iterpolation,
                                df_results, sub_idx, ancil.psm_scale_factor,
                                ancil.source_index, ancil.target_index,
                                ancil.temporal_interp_method))
                        results = client.gather(futures)
                else:
                    logger.info('Running temporal interp in serial.')
                    results = [ancil.temporal_iterpolation(df_results,
                                                           sub_idx)]

                # reconstruct in correct spatial order
                data = []
                for i in range(len(sub_chunks)):
                    data.append(results[i].as_matrix())
                data = np.hstack(data)
                # summary stats
                summary.append(data.mean(axis=0))

                with h5py.File(outHDF, 'a') as f:
                    # write results to HDF
                    f[ancil.psm_name][ancil.output_range,
                                      idx.min():idx.max() + 1] = \
                        data.astype(ancil.psm_dtype)

                logger.info('Temporal interpolation for chunk #{} took {} min'
                            .format(n_chunk, (time.time() - e1) / 60.0))
            except Exception as e:
                _, _, exc_tb = sys.exc_info()
                logger.exception('Error: {e} At Line: {f}.'
                                 .format(e=e, f=exc_tb.tb_lineno))
        if not summary:
            # raise error if summary is empty. Common error.
            raise Exception('Data summary variable could not be created. '
                            'Error in temporal interpolation. Results list: {}'
                            .format(results))

        with h5py.File(outHDF, 'a') as f:
            # save the summary data to csv.
            pd.DataFrame({'lat': f['meta']['latitude'][:],
                          'lon': f['meta']['longitude'][:],
                          'data': np.concatenate(summary)}).to_csv(
                              config['ancillary']['out_dir'] +
                              '{v}_{r}_{y}.csv'.format(v=ancil.psm_name,
                                                       r=region, y=year))
            # store summary in the HDF
            f['meta']['mean_var'] = np.concatenate(summary)

    logger.info('MERRA2 finished for "{}". Total Time: {} min'
                .format(var, (time.time() - t1) / 60.))


def peregrine_merra(config_file, var, year_range, regions=('east', 'west'),
                    queue='batch-h', cores=24, alloc='pxs', log_level='DEBUG',
                    stdout_path='/scratch/gbuster/merra2/'):
    """Run merra on a single variable e+w for a range of years on Peregrine.

    Parameters
    ----------
    config_file : str
        Ancillary variable processing config file with path.
    var : str
        Target variable to process. This must be the MERRA2 variable name,
        not the NSRDB name.
    year_range : iterable
        Year range to be blended. Each year will be a seperate job on
        peregrine. Note that if this is a python range() object, the starting
        index is inclusive and the finishing index is exclusive.
    regions : list | tuple
        Regions to process (east, west).
    queue : str
        Target Peregrine job queue.
    cores : int
        Cores to utilize on the Peregrine node (this is not part of the node
        request).
    alloc : str
        Peregrine project allocation (pxs).
    log_level : str
        Logging level for this module (DEBUG or INFO).
    stdout_path : str
        Path to dump stdout/stderr files.
    """

    for year in year_range:
        for region in regions:
            name = '{year}{region}_{var}'.format(year=str(year)[-2:],
                                                 region=str(region)[0],
                                                 var=var)

            date_range = '[{}0101, {}0101]'.format(year, year + 1)

            cmd = ('python -c '
                   '\'from nsrdb.merra2 import run_single; '
                   'from nsrdb.utilities.loggers import init_logger; '
                   'init_logger("{log_name}", log_level="{log_level}", '
                   'log_file=None); '
                   'run_single("{config_file}", {date_range}, '
                   '"{region}", "{var}", cores={cores})\''
                   )

            cmd = cmd.format(log_name=__name__, log_level=log_level,
                             config_file=config_file, date_range=date_range,
                             region=region, var=var, cores=cores)

            pbs = PBS(cmd, alloc=alloc, queue=queue, name=name,
                      stdout_path=stdout_path)

            print('\ncmd:\n{}\n'.format(cmd))

            if pbs.id:
                msg = ('Kicked off job "{}" (PBS jobid #{}) on '
                       'Peregrine.'.format(name, pbs.id))
            else:
                msg = ('Was unable to kick off job "{}". '
                       'Please see the stdout error messages'
                       .format(name))
            print(msg)
